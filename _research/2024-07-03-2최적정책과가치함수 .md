---
layout: single
title: "최적정책과가치함수"
categories: 파이썬으로 만드는 인공지능
tag: [MCTS]
---


# 9장 강화학습과 게임지능

## 9.2 최적 정책과 가치 함수

### 9.2.1 강화학습과 지도학습의 비교
1. 지도학습
    - training data를 이용
    - 목표 : NN의 출력과 정답 간의 오차 최소화
    - 찾는 것 : 오차를 최소화하는 NN의 **가중치**
    - 해결방법 : SGD algorithm, Adam, Adagrad 등을 이용
2. 강화학습
    - episode를 이용
    - 목표 : 누적 reward을 최대화
    - 찾는 것 : 누적 reward를 최대화하는 **최적정책**
    - 정책의 최적정도는 가치함수를 통해 판단
    - 해결방법 : 동적 프로그래밍, Sarsa, Q learning, DQN


### 9.2.2 최적정책
1. 정책 $\pi$
    - 확률로 표현
    - 상태 $s$에서 어떤 행동 $a$을 취할지 결정하는 확률 분포
    - $\pi(a \mid s) = P(a \mid s), \forall s \in S, \forall a \in A$
    - $ P(a=i|s=j) $ : $j$ state에서 $a$ action을 할 확률


### 9.2.3 가치함수로 찾는 최적정책


2. 가치함수 $\nu_\pi(s)$
    - 정책의 품질을 평가하는 함수
    - 상태 $s$에서 정책 π가 선택할 수 있는 모든 행동 $a$가 가져올 보상의 기대값
    - 가치함수를 최대화하는 최적 정책($\hat{\pi}$)을 찾는 것이 목표 $$\hat{\pi} = \arg\max_{\pi} {\nu}_{\pi}(s), \forall s \in S$$



3. 가치함수 계산 방법
    - 상태 $S$에서 출발하는 모든 episode $z$에 대해 발생하는 확률 $P(z)$와 $r(z)$의 곱들의 합
$$
v_{\pi}(s) = \mathbb{E}(r | s) = \sum_{s \text{에서 출발하는 모든 에피소드} z} P(z) r(z), \forall s \in S
$$


### 9.2.4 가치 함수 계산을 위한 벨만 방정식

1. 벨만 방정식
    - episode가 무한대인 문제를 풀어야하기 때ansdp 모든 에피소드를 나열하는 방식으로 가치함수를 계산할 순 없음
    - 상태들이 서로 밀접하게 관련되어 있다는 사실을 기반으로 순환식인 벨만 방정식을 사용하면, 중복 계산을 피해 계산량을 대폭 줄일 수 있음

    - 다음 state의 가치함수를 알고 있다면 그 값을 지금 state의 r에 더해 총 reward로써 계산할 수 있음을 의미.
$$
{\nu}_{\pi}(s) = \sum_{a \in A(s)} P(a \mid s) \left( r + {\nu}_{\pi}(s') \right), \forall s \in S
$$


5. 할인률을 적용한 벨만 기대 방정식
    - 할인률 ${\gamma}$
        - [0,1] 사이의 실수
        - 먼 미래에 발생하는 reward일 수록 일정 비율로 reward를 삭감하는 역할
        - ${\gamma}=0$ : 미래를 고려하지 않은 근시안적인 추정
        - ${\gamma}=1$ : 미래에 발생하는 모든 보상을 같은 가중치로 평가
    $$
    r(z) = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \gamma^3 r_{t+4} + \cdots + \gamma^{T-(t+1)} r_T
    $$

    이때, 
    $$
    z = (s_t, r_t) \rightarrow (s_{t+1}, r_{t+1}) \rightarrow (s_{t+2}, r_{t+2}) \rightarrow \cdots \rightarrow (s_T, r_T)
    $$


    - 할인률을 적용한 벨만 기대 방정식

    $$
    {\nu}_{\pi}(s) = \sum_{a \in A(s)} P(a \mid s) \left( r + \gamma {\nu}_{\pi}(s') \right), \forall s \in S
    $$
    - 동적 프로그래밍, Sarsa, Q learning 등의 최적 정책을 찾는 학습 algorithm은 벨만 기대 방정식을 어떻게 효과적으로 풀 것인가에 차이가 있음



6. 상태가치함수 | 행동가치함수
    - 가치함수를 상태 $s$와 행동 $a$ 각각의 함수로 나타낼 필요가 있음

    - 상태가치함수 ${\nu}_{\pi}(s)$
        - 특정 정책 ${\pi}$하에서 상태 $s$에서 시작하여 앞으로 얻을 수 있는 기대 리턴
    $$
    {\nu}_{\pi}(s) = \sum_{a \in A(s)} P(a \mid s) \left( r + \gamma {\nu}_{\pi}(s') \right), \forall s \in S
    $$
    

    - 행동가치함수 $q_{\pi}(s, a)$
        - 상태 $s$에서 행동 $a$를 취한 뒤, 이후 정책${\pi}$을 따랐을 때 얻을 수 있는 기대 리턴
        - 이미 행동을 취해서 얻은 $r$에다가 다음 행동으로 얻을 수 있는 상태가치함수를 더한 값
    $$
    q_{\pi}(s, a) = r + \gamma {\nu}_{\pi}(s') = r + \gamma \sum_{a' \in A(s')} P(a' \mid s') q_{\pi}(s', a')
    $$

    - 두 함수의 관계
        - 행동가치함수는 어떤 상태에서 어떤 행동을 한 뒤 얻을 수 있는 기대 보상이다.
        - 어떤 상태에서 할 수 있는 행동이 여러가지이다.
        - 모든 행동으로 얻은 기대보상과 행동을 할 확률을 모두 합하면 한 상태에서 시작하여 얻을 수 있는 기대 보상을 얻을 수 있기 때문에 다음과 같이 나타낼 수 있다.
    $$
    {\nu}_{\pi}(s) = \sum_{a \in A(s)} P(a \mid s)  q_{\pi}(s, a)
    $$